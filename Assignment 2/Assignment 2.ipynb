{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries"
      ],
      "metadata": {
        "id": "hLkXcf31Luhg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWLjHEqVvM3L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.activations import relu, selu\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "OA2K1Sm7MAe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load the heart.csv dataset\n",
        "data = pd.read_csv('heart.csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "X = data.drop(columns=['target'])\n",
        "y = data['target']"
      ],
      "metadata": {
        "id": "VLZTEaXbLCb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset"
      ],
      "metadata": {
        "id": "PmaDf4Qapq_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "E9p_T7rBLD9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale the features"
      ],
      "metadata": {
        "id": "MiPmpZsiEKa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "p5psz6yDEOwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the hyperparameter grid"
      ],
      "metadata": {
        "id": "5eCLfin8p5tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the hyperparameters as shown in the image\n",
        "experiments = [\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'activation_function': 'relu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'activation_function': 'relu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'activation_function': 'selu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.001, 'batch_size': 32, 'activation_function': 'selu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.001, 'batch_size': 64, 'activation_function': 'relu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.001, 'batch_size': 64, 'activation_function': 'relu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.001, 'batch_size': 64, 'activation_function': 'selu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.001, 'batch_size': 64, 'activation_function': 'selu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 32, 'activation_function': 'relu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 32, 'activation_function': 'relu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 32, 'activation_function': 'selu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 32, 'activation_function': 'selu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 64, 'activation_function': 'relu', 'dropout_rate': 0.2},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 64, 'activation_function': 'relu', 'dropout_rate': 0.5},\n",
        "    {'learning_rate': 0.0001, 'batch_size': 64, 'activation_function': 'selu', 'dropout_rate': 0.2},\n",
        "]"
      ],
      "metadata": {
        "id": "oNoyG_IWLQtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "id": "MuChNTevEkzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Function to create and compile the model\n",
        "def create_model(learning_rate, activation_function, dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, input_dim=X_train.shape[1], activation=activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(64, activation=activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "p8J0AljzErAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "bY8DWUW2p-vC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Iterate through the experiments and train the model\n",
        "results = []\n",
        "for experiment in experiments:\n",
        "    model = create_model(\n",
        "        learning_rate=experiment['learning_rate'],\n",
        "        activation_function=experiment['activation_function'],\n",
        "        dropout_rate=experiment['dropout_rate']\n",
        "    )\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=experiment['batch_size'], verbose=0)\n",
        "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "    results.append({\n",
        "        'experiment': experiment,\n",
        "        'accuracy': accuracy\n",
        "    })"
      ],
      "metadata": {
        "id": "K2OnPS-1LWx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d31464-2a14-43f8-f43a-6fe58293e265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7b31bf5b2320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7b31bf1184c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results"
      ],
      "metadata": {
        "id": "XJpQVjb4qCHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Display the results\n",
        "results_df = pd.DataFrame(results)\n",
        "pd.set_option('display.precision', 10)\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "Fj6fud_Ea9sl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a33504-7d5b-42ef-8d22-5b0a9d23a157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           experiment      accuracy\n",
            "0   {'learning_rate': 0.001, 'batch_size': 32, 'ac...  0.8360655904\n",
            "1   {'learning_rate': 0.001, 'batch_size': 32, 'ac...  0.8852459192\n",
            "2   {'learning_rate': 0.001, 'batch_size': 32, 'ac...  0.8524590135\n",
            "3   {'learning_rate': 0.001, 'batch_size': 32, 'ac...  0.8524590135\n",
            "4   {'learning_rate': 0.001, 'batch_size': 64, 'ac...  0.8688524365\n",
            "5   {'learning_rate': 0.001, 'batch_size': 64, 'ac...  0.8688524365\n",
            "6   {'learning_rate': 0.001, 'batch_size': 64, 'ac...  0.8524590135\n",
            "7   {'learning_rate': 0.001, 'batch_size': 64, 'ac...  0.8524590135\n",
            "8   {'learning_rate': 0.0001, 'batch_size': 32, 'a...  0.8524590135\n",
            "9   {'learning_rate': 0.0001, 'batch_size': 32, 'a...  0.8524590135\n",
            "10  {'learning_rate': 0.0001, 'batch_size': 32, 'a...  0.8852459192\n",
            "11  {'learning_rate': 0.0001, 'batch_size': 32, 'a...  0.8524590135\n",
            "12  {'learning_rate': 0.0001, 'batch_size': 64, 'a...  0.8196721077\n",
            "13  {'learning_rate': 0.0001, 'batch_size': 64, 'a...  0.7868852615\n",
            "14  {'learning_rate': 0.0001, 'batch_size': 64, 'a...  0.8852459192\n"
          ]
        }
      ]
    }
  ]
}